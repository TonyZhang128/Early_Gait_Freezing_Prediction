"""
æ­¥æ€è¯†åˆ«å¯¹æ¯”å­¦ä¹ è®­ç»ƒè„šæœ¬
åŠŸèƒ½ï¼šä½¿ç”¨SimCLRæ¡†æ¶è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæ”¯æŒæ•°æ®å¢å¼ºã€TensorBoardç›‘æ§ã€å‚æ•°åŒ–é…ç½®
"""

import os
import argparse
import random
import numpy as np
import scipy.io as sio
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter

from models.resnet import ResNet18
from models.GSDNN_new import GSDNN_new
from models.Conformer import Conformer


# ============== é…ç½®å’Œå‚æ•°è§£ææ¨¡å— ==============

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œéµå¾ªKISSåŸåˆ™ï¼šé…ç½®é›†ä¸­ç®¡ç†"""
    parser = argparse.ArgumentParser(description='æ­¥æ€è¯†åˆ«å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ')
    parser.add_argument('--exp_name', type=str, default='Gait_self_supervised_training')
    parser.add_argument('--mode', type=str, default='debug', help='normal, debug')

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, default='datasets/data_10000/all_data.mat', help='æ•°æ®è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=0, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--views', type=int, default=2, help='å¯¹æ¯”å­¦ä¹ çš„è§†å›¾æ•°é‡')

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_type", type=str, default="GSDNN", help="æ¨¡å‹ç±»å‹, å¯ä»¥é€‰GSDNN,ResNet,EEGNet,Conformer")
    ## parameters for GSDNN
    parser.add_argument('--num_classes', type=int, default=1, help='è¾“å‡ºç±»åˆ«æ•°')
    parser.add_argument('--block_n', type=int, default=8, help='æ¨¡å—å †å æ¬¡æ•°')
    parser.add_argument('--init_channels', type=int, default=18, help='æ•°æ®è¾“å…¥ç»´åº¦')
    parser.add_argument('--growth_rate', type=int, default=12, help='æ¨¡å—æ¯å ä¸€æ¬¡ï¼Œç»´åº¦æå‡å¤šå°‘')
    parser.add_argument('--base_channels', type=int, default=48, help='åˆå§‹ç‰¹å¾ç»´åº¦')
    parser.add_argument('--stride', type=int, default=2, help='å·ç§¯æ­¥é•¿')
    parser.add_argument('--dropout_GSDNN', type=float, default=0.2, help='GSDNNä¸¢å¤±æ¦‚ç‡')
    
    ## parameters for projection head
    ### GSDNN [132 128 256]
    ### ResNet18 [64 128 256]
    parser.add_argument('--out_dim', type=int, default=132, help='ç¼–ç å™¨è¾“å‡ºç»´åº¦')
    parser.add_argument('--proj_out_dim', type=int, default=128, help='æŠ•å½±å¤´ä¸­é—´å±‚ç»´åº¦')
    parser.add_argument('--contrastive_dim', type=int, default=256, help='è¿›è¡Œå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾ç©ºé—´ç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.5, help='Dropoutæ¦‚ç‡')
    

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--temperature', type=float, default=0.5, help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°')

    # æ•°æ®å¢å¼ºå‚æ•°
    parser.add_argument('--freq_keep_ratio', type=float, default=0.6, help='é¢‘ç‡dropoutä¿ç•™æ¯”ä¾‹')

    # ä¿å­˜å’Œæ—¥å¿—å‚æ•°
    parser.add_argument('--save_dir', type=str, default='./save_models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='./runs', help='TensorBoardæ—¥å¿—ç›®å½•')
    parser.add_argument('--save_freq', type=int, default=5, help='æ¨¡å‹ä¿å­˜é¢‘ç‡ï¼ˆæ¯nä¸ªepochï¼‰')

    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', type=str, default=None, help='è®­ç»ƒè®¾å¤‡ï¼ˆcuda/cpuï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')

    args = parser.parse_args()
    
    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if args.device is None:
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # æ‰“å°æ‰€æœ‰é…ç½®ä¿¡æ¯
    print("="*70)
    print("ğŸ“Š æ­¥æ€è¯†åˆ«å¯¹æ¯”å­¦ä¹ è®­ç»ƒé…ç½®ä¿¡æ¯")
    print("="*70)
    
    # åŸºç¡€é…ç½®
    print("\n[åŸºç¡€é…ç½®]")
    print(f"  å®éªŒåç§°      (exp_name): {args.exp_name}")
    print(f"  è¿è¡Œæ¨¡å¼      (mode): {args.mode}")
    print(f"  éšæœºç§å­      (seed): {args.seed}")
    
    # æ•°æ®å‚æ•°
    print("\n[æ•°æ®å‚æ•°]")
    print(f"  æ•°æ®è·¯å¾„      (data_path): {args.data_path}")
    print(f"  æ‰¹æ¬¡å¤§å°      (batch_size): {args.batch_size}")
    print(f"  åŠ è½½çº¿ç¨‹æ•°    (num_workers): {args.num_workers}")
    print(f"  å¯¹æ¯”è§†å›¾æ•°    (views): {args.views}")
    
    # æ¨¡å‹å‚æ•°
    print("\n[æ¨¡å‹å‚æ•°]")
    print(f"  æ¨¡å‹ç±»å‹      (model_type): {args.model_type}")
    print(f"  è¾“å‡ºç±»åˆ«æ•°    (num_classes): {args.num_classes}")
    print(f"  GSDNNæ¨¡å—å †å æ¬¡æ•° (block_n): {args.block_n}")
    print(f"  æ•°æ®è¾“å…¥ç»´åº¦  (init_channels): {args.init_channels}")
    print(f"  ç»´åº¦å¢é•¿é€Ÿç‡  (growth_rate): {args.growth_rate}")
    print(f"  åˆå§‹ç‰¹å¾ç»´åº¦  (base_channels): {args.base_channels}")
    print(f"  å·ç§¯æ­¥é•¿      (stride): {args.stride}")
    print(f"  GSDNN dropout (dropout_GSDNN): {args.dropout_GSDNN}")
    print(f"  ç¼–ç å™¨è¾“å‡ºç»´åº¦ (out_dim): {args.out_dim}")
    print(f"  æŠ•å½±å¤´ä¸­é—´ç»´åº¦ (proj_out_dim): {args.proj_out_dim}")
    print(f"  å¯¹æ¯”å­¦ä¹ ç»´åº¦  (contrastive_dim): {args.contrastive_dim}")
    print(f"  Dropoutæ¦‚ç‡   (dropout): {args.dropout}")
    
    # è®­ç»ƒå‚æ•°
    print("\n[è®­ç»ƒå‚æ•°]")
    print(f"  è®­ç»ƒè½®æ•°      (epochs): {args.epochs}")
    print(f"  å­¦ä¹ ç‡        (lr): {args.lr}")
    print(f"  æ¸©åº¦å‚æ•°      (temperature): {args.temperature}")
    
    # æ•°æ®å¢å¼ºå‚æ•°
    print("\n[æ•°æ®å¢å¼ºå‚æ•°]")
    print(f"  é¢‘ç‡ä¿ç•™æ¯”ä¾‹  (freq_keep_ratio): {args.freq_keep_ratio}")
    
    # ä¿å­˜å’Œæ—¥å¿—å‚æ•°
    print("\n[ä¿å­˜å’Œæ—¥å¿—å‚æ•°]")
    print(f"  æ¨¡å‹ä¿å­˜ç›®å½•  (save_dir): {args.save_dir}")
    print(f"  æ—¥å¿—ç›®å½•      (log_dir): {args.log_dir}")
    print(f"  ä¿å­˜é¢‘ç‡      (save_freq): {args.save_freq}")
    
    # è®¾å¤‡å‚æ•°
    print("\n[è®¾å¤‡å‚æ•°]")
    print(f"  è®­ç»ƒè®¾å¤‡      (device): {args.device}")
    if args.device == 'cuda':
        print(f"  GPUæ•°é‡       : {torch.cuda.device_count()}")
        print(f"  å½“å‰GPU       : {torch.cuda.get_device_name(0)}")
    
    print("\n" + "="*70)
    
    return args


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


# ============== æ•°æ®å¢å¼ºæ¨¡å— ==============

class GaitAugmentation:
    """æ­¥æ€æ•°æ®å¢å¼ºç±»ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™"""

    @staticmethod
    def reverse_time_series(data):
        """æ—¶é—´åºåˆ—åè½¬ï¼ˆåŠ è´Ÿå·ï¼‰"""
        return -data

    @staticmethod
    def random_channel_shuffle(data):
        """éšæœºé€šé“æ‰“ä¹±"""
        assert data.dim() == 3, "Input data must be 3D with channels as the second dimension"
        num_channels = data.size(1)
        shuffled_indices = torch.randperm(num_channels)
        return data[:, shuffled_indices, :]

    @staticmethod
    def random_frequency_dropout(data, keep_ratio=0.6):
        """éšæœºé¢‘ç‡dropout"""
        fft_img = torch.fft.fftn(data, dim=2)
        magnitude = torch.abs(fft_img)
        num_freqs = magnitude.shape[2]
        keep_indices = np.random.choice(num_freqs, int(num_freqs * keep_ratio), replace=False)
        mask = torch.zeros_like(magnitude, dtype=torch.bool)
        # keep_indices = torch.from_numpy(keep_indices).to(data.device)  # è½¬torchå¼ é‡+å¯¹é½è®¾å¤‡
        mask[:, :, keep_indices] = 1
        fft_img = fft_img * mask
        img = torch.fft.ifftn(fft_img, dim=2)
        return torch.real(img)

    @classmethod
    def get_transforms(cls, freq_keep_ratio=0.6):
        """è·å–æ•°æ®å¢å¼ºå˜æ¢ç»„åˆ"""
        return transforms.Compose([
            transforms.RandomApply([
                # å¼±å¢å¼ºæ“ä½œ
                transforms.RandomResizedCrop((18, 101), scale=(0.3, 0.8)), # , antialias=True
                transforms.RandomErasing(p=0.5, scale=(0.2, 0.4), ratio=(0.3, 3.3), value=0),
                transforms.Lambda(lambda x: cls.random_frequency_dropout(x, freq_keep_ratio)),
                # å¼ºå¢å¼ºæ“ä½œ
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.Lambda(cls.random_channel_shuffle),
                transforms.Lambda(cls.reverse_time_series)
            ], p=1.0)
        ]) # trasform æ‰§è¡Œçš„æ¦‚ç‡æ˜¯ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„è¶…å‚æ•°


# ============== æ•°æ®é›†æ¨¡å— ==============

class ContrastiveDataset(Dataset):
    """å¯¹æ¯”å­¦ä¹ æ•°æ®é›†ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™"""

    def __init__(self, data_array, data_transform, views=2):
        """
        Args:
            data_array: numpyæ•°ç»„æ ¼å¼çš„æ­¥æ€æ•°æ®
            data_transform: æ•°æ®å¢å¼ºå˜æ¢
            views: å¯¹æ¯”å­¦ä¹ çš„è§†å›¾æ•°é‡
        """
        self.transform = data_transform
        self.data_array = data_array
        self.views = views

    def __len__(self):
        return len(self.data_array)

    def __getitem__(self, idx):
        """è·å–å¢å¼ºåçš„æ ·æœ¬å¯¹"""
        img = self.data_array[idx]
        img = torch.tensor(np.expand_dims(img, axis=0))
        imgs = []
        for _ in range(self.views):
            img_aug = self.transform(img)
            imgs.append(img_aug)
        return imgs


def load_data(data_path, batch_size, views, num_workers, transform):
    """åŠ è½½æ•°æ®å¹¶åˆ›å»ºDataLoader"""
    gait = sio.loadmat(data_path)['all_data']
    dataset = ContrastiveDataset(data_array=gait, data_transform=transform, views=views)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    return dataloader


# ============== æ¨¡å‹å®šä¹‰æ¨¡å— ==============

class SimCLRModel(nn.Module):
    """SimCLRå¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™"""

    def __init__(self, base_model, out_dim=32, proj_out_dim=128, contrastive_dim=256, dropout=0.5):
        """
        Args:
            base_model: åŸºç¡€ç¼–ç å™¨ï¼ˆå¦‚ResNetï¼‰
            out_dim: ç¼–ç å™¨è¾“å‡ºç»´åº¦
            proj_out_dim: æŠ•å½±å¤´ä¸­é—´å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super(SimCLRModel, self).__init__()
        self.encoder = nn.Sequential(*list(base_model.children())[:-1])
        self.dropout = nn.Dropout(p=dropout)

        # æŠ•å½±å¤´
        self.projector = nn.Sequential(
            nn.Linear(out_dim, proj_out_dim),
            nn.ReLU(inplace=True),
            nn.Linear(proj_out_dim, contrastive_dim)
        )

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        h = self.encoder(x)
        h = self.dropout(h)
        h = h.view(h.size(0), -1)
        h = self.projector(h)
        return h


class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ˆNT-XentæŸå¤±ï¼‰"""

    def __init__(self, temperature=0.5):
        """
        Args:
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘åº¦
        """
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        """
        Args:
            z_i: ç¬¬ä¸€ä¸ªè§†å›¾çš„æŠ•å½±å‘é‡
            z_j: ç¬¬äºŒä¸ªè§†å›¾çš„æŠ•å½±å‘é‡
        Returns:
            å¯¹æ¯”æŸå¤±å€¼
        """
        batch_size = z_i.shape[0]
        z_i = nn.functional.normalize(z_i, dim=1)
        z_j = nn.functional.normalize(z_j, dim=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(z_i, z_j.T) / self.temperature
        sim_labels = torch.arange(batch_size).to(z_i.device)
        loss = nn.CrossEntropyLoss()(sim_matrix, sim_labels)
        return loss


def create_model(device, model_type, args):
    """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    if model_type == 'GSDNN':
        encoder = GSDNN_new(args.num_classes, args.block_n, args.init_channels, 
                            args.growth_rate, args.base_channels, args.stride, args.dropout_GSDNN)
    elif model_type == 'ResNet':
        encoder = ResNet18()
    elif model_type == 'Conformer':
        encoder = Conformer(emb_size=40, depth=6, n_classes=4)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    model = SimCLRModel(encoder, args.out_dim, args.proj_out_dim, args.contrastive_dim, args.dropout).to(device)
    return model


def create_optimizer(model, lr=3e-4):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    return torch.optim.Adam(model.parameters(), lr=lr)


# ============== è®­ç»ƒå’ŒéªŒè¯æ¨¡å— ==============

def train_epoch(model, dataloader, criterion, optimizer, device, epoch, writer, args):
    """è®­ç»ƒä¸€ä¸ªepoch

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        epoch: å½“å‰epoch
        writer: TensorBoard writer

    Returns:
        å¹³å‡æŸå¤±å€¼
    """
    model.train()
    total_loss = 0.0
    num_batches = 0

    for batch_idx, (img1, img2) in enumerate(dataloader):
        optimizer.zero_grad()
        if args.model_type == 'GSDNN':
            img1 = img1.squeeze(1)
            img2 = img2.squeeze(1)
        
        # å‰å‘ä¼ æ’­
        z_i = model(img1.to(device=device, dtype=torch.float32))
        z_j = model(img2.to(device=device, dtype=torch.float32))

        # è®¡ç®—æŸå¤±
        loss = criterion(z_i, z_j)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

        # è®°å½•åˆ°TensorBoardï¼ˆæ¯10ä¸ªbatchï¼‰
        if batch_idx % 10 == 0:
            global_step = epoch * len(dataloader) + batch_idx
            writer.add_scalar('Loss/train_batch', loss.item(), global_step)

        if args.mode == 'debug':
            break
    avg_loss = total_loss / num_batches
    return avg_loss


def save_checkpoint(model, save_dir, filename, epoch, loss, is_best=False):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹

    Args:
        model: æ¨¡å‹
        save_dir: ä¿å­˜ç›®å½•
        filename: æ–‡ä»¶å
        epoch: å½“å‰epoch
        loss: å½“å‰æŸå¤±
        is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
    """
    os.makedirs(save_dir, exist_ok=True)
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'loss': loss,
    }

    # ä¿å­˜æ£€æŸ¥ç‚¹
    save_path = os.path.join(save_dir, filename)
    torch.save(checkpoint, save_path)

    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜
    if is_best:
        best_path = os.path.join(save_dir, 'best_model.pth')
        torch.save(checkpoint, best_path)
        print(f'Epoch {epoch + 1}, New minimum loss: {loss:.6f}, Best model saved.')


def plot_loss_curve(loss_values, save_dir):
    """ç»˜åˆ¶æŸå¤±æ›²çº¿

    Args:
        loss_values: æŸå¤±å€¼åˆ—è¡¨
        save_dir: ä¿å­˜ç›®å½•
    """
    plt.figure(figsize=(10, 5))
    plt.plot(loss_values, label='Training Loss')
    plt.title('Training Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # ä¿å­˜å›¾åƒ
    save_path = os.path.join(save_dir, 'loss_curve.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f'Loss curve saved to {save_path}')
    plt.close()


# ============== ä¸»è®­ç»ƒæµç¨‹ ==============

def train(args):
    """ä¸»è®­ç»ƒå‡½æ•°ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    # 1. è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    # 2. è®¾ç½®è®¾å¤‡
    if args.device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    print(f'Using device: {device}')

    # 3. åˆ›å»ºå®éªŒç›®å½•
    if args.exp_name is None:
        args.exp_name = datetime.now().strftime('%Y%m%d_%H%M%S')
    # else:
    #     args.exp_name = args.exp_name + datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = os.path.join(args.log_dir, args.exp_name)
    save_dir = os.path.join(args.save_dir, args.exp_name)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # 4. åˆ›å»ºTensorBoard writer
    writer = SummaryWriter(log_dir)
    print(f'TensorBoard logs will be saved to: {log_dir}')
    print(f'Models will be saved to: {save_dir}')

    # 5. å‡†å¤‡æ•°æ®
    print('Loading data...')
    transform = GaitAugmentation.get_transforms(args.freq_keep_ratio)
    dataloader = load_data(
        data_path=args.data_path,
        batch_size=args.batch_size,
        views=args.views,
        num_workers=args.num_workers,
        transform=transform
    )
    print(f'Data loaded. Total batches: {len(dataloader)}. Total train data: {len(dataloader)*args.batch_size}')

    # 6. åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    print('Creating model...')
    model = create_model(device, args.model_type, args)
    criterion = ContrastiveLoss(temperature=args.temperature)
    optimizer = create_optimizer(model, args.lr)

    # 7. è®­ç»ƒå¾ªç¯
    print('Starting training...')
    loss_values = []
    min_loss = float('inf')

    for epoch in range(args.epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_epoch(model, dataloader, criterion, optimizer, device, epoch, writer, args)
        loss_values.append(avg_loss)

        # è®°å½•åˆ°TensorBoard
        writer.add_scalar('Loss/train_epoch', avg_loss, epoch)

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f'Epoch {epoch + 1}/{args.epochs}, Average Loss: {avg_loss:.6f}')

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        is_best = avg_loss < min_loss
        if is_best:
            min_loss = avg_loss

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_freq == 0 or is_best:
            filename = f'checkpoint_epoch_{epoch + 1}.pth'
            save_checkpoint(model, save_dir, filename, epoch, avg_loss, is_best)

    # 8. è®­ç»ƒå®Œæˆ
    print('Training completed!')
    print(f'Minimum loss: {min_loss:.6f}')

    # 9. ç»˜åˆ¶æŸå¤±æ›²çº¿
    # plot_loss_curve(loss_values, save_dir)

    # 10. å…³é—­TensorBoard writer
    writer.close()

    print(f'All results saved to: {save_dir}')
    print(f"View TensorBoard logs with: tensorboard --logdir={log_dir}")


if __name__ == '__main__':
    # è§£æå‚æ•°
    args = parse_args()

    # å¼€å§‹è®­ç»ƒ
    train(args)
