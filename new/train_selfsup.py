"""
æ­¥æ€è¯†åˆ«å¯¹æ¯”å­¦ä¹ è®­ç»ƒè„šæœ¬
åŠŸèƒ½ï¼šä½¿ç”¨SimCLRæ¡†æ¶è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæ”¯æŒæ•°æ®å¢å¼ºã€TensorBoardç›‘æ§ã€å‚æ•°åŒ–é…ç½®
"""

import os
import argparse
from datetime import datetime
import torch
import torch.nn as nn
from torch.optim import SGD
from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR
from torch.utils.tensorboard import SummaryWriter

from datasets.Contrastive_dataset import load_data

from models.resnet import ResNet18
from models.GSDNN_new import GSDNN_new
from models.Conformer import Conformer

from utils.analysis import set_seed, save_checkpoint
from utils.calculate import FLOPs_calculat




# ============== æ¨¡å‹å®šä¹‰æ¨¡å— ==============

class SimCLRModel(nn.Module):
    """SimCLRå¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™"""

    def __init__(self, base_model, out_dim=32, proj_out_dim=128, contrastive_dim=256, dropout=0.5):
        """
        Args:
            base_model: åŸºç¡€ç¼–ç å™¨ï¼ˆå¦‚ResNetï¼‰
            out_dim: ç¼–ç å™¨è¾“å‡ºç»´åº¦
            proj_out_dim: æŠ•å½±å¤´ä¸­é—´å±‚ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        super(SimCLRModel, self).__init__()
        self.encoder = nn.Sequential(*list(base_model.children())[:-1])
        self.dropout = nn.Dropout(p=dropout)

        # æŠ•å½±å¤´
        self.projector = nn.Sequential(
            nn.Linear(out_dim, proj_out_dim),
            nn.ReLU(inplace=True),
            nn.Linear(proj_out_dim, contrastive_dim)
        )

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        h = self.encoder(x)
        h = self.dropout(h)
        h = torch.flatten(h, start_dim=1) # h.view(h.size(0), -1)
        h = self.projector(h)
        return h


class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ˆNT-XentæŸå¤±ï¼‰"""

    def __init__(self, temperature=0.5):
        """
        Args:
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘åº¦
        """
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature
        self.eps = 1e-8

    def forward(self, z_i, z_j):
        """
        Args:
            z_i: ç¬¬ä¸€ä¸ªè§†å›¾çš„æŠ•å½±å‘é‡
            z_j: ç¬¬äºŒä¸ªè§†å›¾çš„æŠ•å½±å‘é‡
        Returns:
            å¯¹æ¯”æŸå¤±å€¼
        """
        batch_size = z_i.shape[0]
        z_i = nn.functional.normalize(z_i, dim=1)
        z_j = nn.functional.normalize(z_j, dim=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(z_i, z_j.T) / self.temperature
        sim_labels = torch.arange(batch_size).to(z_i.device)
        loss = nn.CrossEntropyLoss()(sim_matrix, sim_labels)
        return loss


def create_model(device, model_type, args):
    """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    if model_type == 'GSDNN':
        encoder = GSDNN_new(args.num_classes, args.block_n, args.init_channels, 
                            args.growth_rate, args.base_channels, args.stride, args.dropout_GSDNN)
    elif model_type == 'ResNet':
        encoder = ResNet18()
    elif model_type == 'Conformer':
        encoder = Conformer(emb_size=40, depth=6, n_classes=4)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    model = SimCLRModel(encoder, args.out_dim, args.proj_out_dim, args.contrastive_dim, args.dropout).to(device)
    return model


# 3. å®šä¹‰é¢„çƒ­è°ƒåº¦å™¨ï¼ˆçº¿æ€§ä¸Šå‡ï¼‰
def get_warmup_lr_lambda(warmup_steps):
    def warmup_lr_lambda(step):
        return step / warmup_steps if step < warmup_steps else 1.0
    return warmup_lr_lambda

def create_optimizer(model, args, total_steps):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    batch_size = args.batch_size  # å®é™…è®­ç»ƒæ‰¹æ¬¡
    base_lr = args.base_lr     # åŸºç¡€æ‰¹æ¬¡256å¯¹åº”çš„åˆå§‹å­¦ä¹ ç‡
    total_steps = total_steps  # æ€»è®­ç»ƒæ­¥æ•°
    warmup_steps = int(total_steps * args.warmup_ratio)  # é¢„çƒ­æ­¥æ•°ï¼š5%æ€»æ­¥æ•°
    min_lr = base_lr * (batch_size / 256) * 1e-3  # æœ€å°å­¦ä¹ ç‡
    optimizer = SGD(model.parameters(), 
                lr=base_lr * (batch_size / 256),  # çº¿æ€§ç¼©æ”¾å­¦ä¹ ç‡
                momentum=args.momentum, 
                weight_decay=args.weight_decay)
    
    warmup_scheduler = LambdaLR(optimizer, lr_lambda=get_warmup_lr_lambda(warmup_steps))

    # 4. å®šä¹‰ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼ˆé¢„çƒ­åæ‰§è¡Œï¼‰
    cos_scheduler = CosineAnnealingLR(optimizer, 
                                  T_max=total_steps - warmup_steps,  # é€€ç«æ€»æ­¥æ•°
                                  eta_min=min_lr)  # æœ€å°å­¦ä¹ ç‡
    # torch.optim.Adam(model.parameters(), lr=lr)
    return optimizer, warmup_scheduler, cos_scheduler


# ============== è®­ç»ƒå’ŒéªŒè¯æ¨¡å— ==============

def train_epoch(model, dataloader, criterion, optimizer, warmup_scheduler, 
                cos_scheduler, warmup_steps, device, epoch, writer, args):
    """è®­ç»ƒä¸€ä¸ªepoch

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        epoch: å½“å‰epoch
        writer: TensorBoard writer

    Returns:
        å¹³å‡æŸå¤±å€¼
    """
    model.train()
    total_loss = 0.0
    num_batches = 0
    steps_one_epoch = len(dataloader)
    
    for batch_idx, (img1, img2) in enumerate(dataloader):
        optimizer.zero_grad()
        if args.model_type == 'GSDNN':
            img1 = img1.squeeze(1)
            img2 = img2.squeeze(1)
        
        img1, img2 = img1.to(device, dtype=torch.float32), img2.to(device, dtype=torch.float32)
        # å‰å‘ä¼ æ’­
        z_i = model(img1)
        z_j = model(img2)

        # è®¡ç®—æŸå¤±
        loss = criterion(z_i, z_j)

        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦èŒƒæ•°æœ€å¤§ä¸º1.0ï¼Œå¯å¾®è°ƒ
        optimizer.step()
        
         
        step = epoch * steps_one_epoch + batch_idx + 1 
        if step >= warmup_steps: # é¢„çƒ­ç»“æŸåï¼Œå¯åŠ¨ä½™å¼¦é€€ç«
            cos_scheduler.step()
        else:
            warmup_scheduler.step() 

        total_loss += loss.item()
        num_batches += 1

        # è®°å½•åˆ°TensorBoardï¼ˆæ¯10ä¸ªbatchï¼‰
        if batch_idx % 50 == 0:
            global_step = epoch * len(dataloader) + batch_idx
            current_lr = optimizer.param_groups[0]['lr']
            writer.add_scalar('LearningRate/train', current_lr, global_step)
            writer.add_scalar('Loss/train_batch', loss.item(), global_step)

        if args.mode == 'debug':
            break
    avg_loss = total_loss / num_batches
    return avg_loss


# ============== ä¸»è®­ç»ƒæµç¨‹ ==============

def train(args):
    """ä¸»è®­ç»ƒå‡½æ•°ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    # 1. è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    # æ€§èƒ½ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True  # é’ˆå¯¹å›ºå®šè¾“å…¥å°ºå¯¸ï¼ŒåŠ é€Ÿå·ç§¯è®¡ç®—
    torch.backends.cudnn.deterministic = False  # å…³é—­ç¡®å®šæ€§ï¼Œæå‡é€Ÿåº¦ï¼ˆè‹¥éœ€ä¸¥æ ¼å¤ç°ï¼Œè®¾ä¸ºTrueï¼‰
    torch.cuda.empty_cache()  # æ¸…ç©ºGPUç¼“å­˜

    # 2. è®¾ç½®è®¾å¤‡
    if args.device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    print(f'Using device: {device}')

    # 3. åˆ›å»ºå®éªŒç›®å½•        
    # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # log_dir = os.path.join(args.log_dir, args.exp_name + '_' + timestamp)
    # save_dir = os.path.join(args.save_dir, args.exp_name + '_' + timestamp)
    log_dir = args.log_dir
    save_dir = args.save_dir
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)

    # 4. åˆ›å»ºTensorBoard writer
    writer = SummaryWriter(log_dir)
    # print(f'TensorBoard logs will be saved to: {log_dir}')
    # print(f'Models will be saved to: {save_dir}')

    # 5. å‡†å¤‡æ•°æ®
    print('Loading data...')
    
    dataloader = load_data(
        data_path=args.data_path,
        batch_size=args.batch_size,
        views=args.views,
        num_workers=args.num_workers,
        args=args
    )
    print(f'Data loaded. Total batches: {len(dataloader)}. Total train data: {len(dataloader.dataset)}')
     
    # 6. åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    print(f'Creating model...Encoder use {args.model_type}')
    model = create_model(device, args.model_type, args)
    data_shape = [1, 18, 101]
    FLOPs_calculat(model, device, data_shape)
    criterion = ContrastiveLoss(temperature=args.temperature).to(device)
    
    total_steps = args.epochs * len(dataloader) 
    warmup_steps = int(total_steps * args.warmup_ratio)
    optimizer, warmup_scheduler, cos_scheduler = create_optimizer(model, args, total_steps)

    # 7. è®­ç»ƒå¾ªç¯
    print('Starting training...')
    loss_values = []
    min_loss = float('inf')

    for epoch in range(args.epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_epoch(model, dataloader, criterion, optimizer, warmup_scheduler, 
                               cos_scheduler, warmup_steps, device, epoch, writer, args)
        loss_values.append(avg_loss)

        # è®°å½•åˆ°TensorBoard
        writer.add_scalar('Loss/train_epoch', avg_loss, epoch)

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f'Epoch {epoch + 1}/{args.epochs}, Average Loss: {avg_loss:.6f}')

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        is_best = avg_loss < min_loss
        if is_best:
            min_loss = avg_loss

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_freq == 0 or is_best:
            filename = f'checkpoint_epoch_{epoch + 1}.pth'
            save_checkpoint(model, save_dir, filename, epoch, avg_loss, is_best)

    # 8. è®­ç»ƒå®Œæˆ
    print('Training completed!')
    print(f'Minimum loss: {min_loss:.6f}')

    # 9. ç»˜åˆ¶æŸå¤±æ›²çº¿
    # plot_loss_curve(loss_values, save_dir)

    # 10. å…³é—­TensorBoard writer
    writer.close()

    print(f'All results saved to: {save_dir}')
    print(f"View TensorBoard logs with: tensorboard --logdir={log_dir}")

# ============== é…ç½®å’Œå‚æ•°è§£ææ¨¡å— ==============

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œéµå¾ªKISSåŸåˆ™ï¼šé…ç½®é›†ä¸­ç®¡ç†"""
    parser = argparse.ArgumentParser(description='æ­¥æ€è¯†åˆ«å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ')
    parser.add_argument('--exp_name', type=str, default='Gait_selfsup_GSDNN_baseline')
    parser.add_argument('--mode', type=str, default='debug', help='normal, debug')
    parser.add_argument('--print_params', type=bool, default=True, help='æ‰“å°å‚æ•°')

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, default='datasets/data_10000/all_data.mat', help='æ•°æ®è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=256, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=0, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--views', type=int, default=2, help='å¯¹æ¯”å­¦ä¹ çš„è§†å›¾æ•°é‡')

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_type", type=str, default="GSDNN", help="æ¨¡å‹ç±»å‹, å¯ä»¥é€‰GSDNN,ResNet,EEGNet,Conformer")
    ## parameters for GSDNN
    parser.add_argument('--num_classes', type=int, default=1, help='è¾“å‡ºç±»åˆ«æ•°')
    parser.add_argument('--block_n', type=int, default=8, help='æ¨¡å—å †å æ¬¡æ•°')
    parser.add_argument('--init_channels', type=int, default=18, help='æ•°æ®è¾“å…¥ç»´åº¦')
    parser.add_argument('--growth_rate', type=int, default=12, help='æ¨¡å—æ¯å ä¸€æ¬¡ï¼Œç»´åº¦æå‡å¤šå°‘')
    parser.add_argument('--base_channels', type=int, default=48, help='åˆå§‹ç‰¹å¾ç»´åº¦')
    parser.add_argument('--stride', type=int, default=2, help='å·ç§¯æ­¥é•¿')
    parser.add_argument('--dropout_GSDNN', type=float, default=0.2, help='GSDNNä¸¢å¤±æ¦‚ç‡')
    
    ## parameters for projection head
    ### GSDNN [132 128 256]
    ### ResNet18 [64 128 256]
    parser.add_argument('--out_dim', type=int, default=132, help='ç¼–ç å™¨è¾“å‡ºç»´åº¦')
    parser.add_argument('--proj_out_dim', type=int, default=128, help='æŠ•å½±å¤´ä¸­é—´å±‚ç»´åº¦')
    parser.add_argument('--contrastive_dim', type=int, default=256, help='è¿›è¡Œå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾ç©ºé—´ç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.5, help='Dropoutæ¦‚ç‡')
    

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=40, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--base_lr', type=float, default=0.3, help='å­¦ä¹ ç‡')
    parser.add_argument('--warmup_ratio', type=float, default=0.05, help='å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼ˆæ¯”ä¾‹ï¼‰')
    parser.add_argument('--temperature', type=float, default=0.5, help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°')
    
    parser.add_argument('--momentum', type=float, default=0.9, help='SGDåŠ¨é‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡ç³»æ•°')

    # æ•°æ®å¢å¼ºå‚æ•°
    parser.add_argument('--freq_keep_ratio', type=float, default=0.6, help='é¢‘ç‡dropoutä¿ç•™æ¯”ä¾‹')

    # ä¿å­˜å’Œæ—¥å¿—å‚æ•°
    parser.add_argument('--save_dir', type=str, default='./save_models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--log_dir', type=str, default='./runs', help='TensorBoardæ—¥å¿—ç›®å½•')
    parser.add_argument('--save_freq', type=int, default=10, help='æ¨¡å‹ä¿å­˜é¢‘ç‡ï¼ˆæ¯nä¸ªepochï¼‰')

    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', type=str, default=None, help='è®­ç»ƒè®¾å¤‡ï¼ˆcuda/cpuï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')

    args = parser.parse_args()
    
    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if args.device is None:
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    if args.print_params:
        print("="*70)
        print("ğŸ“Š æ­¥æ€è¯†åˆ«å¯¹æ¯”å­¦ä¹ è®­ç»ƒé…ç½®ä¿¡æ¯")
        print("="*70)
        for key, value in sorted(vars(args).items()):
            print(f"  {key.ljust(30)}: {value}")
        print("="*70)
    
    return args

if __name__ == '__main__':
    # è§£æå‚æ•°
    args = parse_args()

    # å¼€å§‹è®­ç»ƒ
    train(args)
