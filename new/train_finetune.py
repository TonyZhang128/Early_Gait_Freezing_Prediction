"""
æ­¥æ€è¯†åˆ«åˆ†ç±»è®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ
åŒ…å«TensorBoardç›‘æ§ã€å‚æ•°è§£æã€æ¨¡å—åŒ–è®¾è®¡
"""

import argparse
import os
from pathlib import Path
from datetime import datetime

import numpy as np
import scipy.io as sio
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.metrics import (confusion_matrix, f1_score, precision_score,
                             recall_score, auc, roc_auc_score)
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms

# å¯¼å…¥æ¨¡å‹
from models.DNN import DNN
from models.GSDNN_new import GSDNN_new
from models.resnet import ResNet18



# ================================ æ•°æ®å¢å¼ºæ¨¡å— ================================

def reverse_time_series(data):
    """æ—¶é—´åºåˆ—åè½¬"""
    return -data


def random_channel_shuffle(data):
    """éšæœºé€šé“æ‰“ä¹±"""
    assert data.dim() == 3, "Input data must be 3D with channels as the second dimension"
    num_channels = data.size(1)
    shuffled_indices = torch.randperm(num_channels)
    return data[:, shuffled_indices, :]


def random_frequency_dropout(img, keep_ratio=0.6):
    """éšæœºé¢‘ç‡æˆåˆ†ä¸¢å¼ƒ"""
    fft_img = torch.fft.fftn(img, dim=2)
    magnitude = torch.abs(fft_img)
    num_freqs = magnitude.shape[2]
    keep_indices = np.random.choice(num_freqs, int(num_freqs * keep_ratio), replace=False)
    mask = torch.zeros_like(magnitude, dtype=torch.bool)
    mask[:, :, keep_indices] = 1
    fft_img = fft_img * mask
    img = torch.fft.ifftn(fft_img, dim=2)
    return torch.real(img)


def get_data_transforms(augmentation_prob=0.5, freq_keep_ratio=0.6):
    """æ„å»ºæ•°æ®å¢å¼ºå˜æ¢ç»„åˆ"""
    return transforms.Compose([
        transforms.RandomApply([
            transforms.Lambda(lambda x: random_frequency_dropout(x, freq_keep_ratio)),
            transforms.Lambda(reverse_time_series),
        ], p=augmentation_prob)
    ])


# ================================ æ•°æ®é›†ç±» ================================

class GaitDataset(Dataset):
    """æ­¥æ€æ•°æ®é›†ç±»"""

    def __init__(self, data_array, data_label, data_transform=None, views=2):
        """
        Args:
            data_array: æ•°æ®æ•°ç»„
            data_label: æ ‡ç­¾æ•°ç»„
            data_transform: æ•°æ®å˜æ¢
            views: è§†è§’æ•°é‡
        """
        self.transform = data_transform
        self.data_array = data_array
        self.data_label = data_label
        self.views = views

    def __len__(self):
        return len(self.data_array)

    def __getitem__(self, idx):
        img = self.data_array[idx]
        if self.transform:
            img = self.transform(torch.tensor(np.expand_dims(img, axis=0)))
        return img, self.data_label[idx]


# ================================ æ•°æ®åŠ è½½æ¨¡å— ================================

def load_and_split_data(data_path, train_ratio=0.8, random_seed=42):
    """
    åŠ è½½å¹¶åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«åç¼€ï¼‰
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        random_seed: éšæœºç§å­

    Returns:
        train_data, test_data, train_label, test_label
    """
    np.random.seed(random_seed)

    # åŠ è½½æ•°æ®
    # data_finetue = sio.loadmat(f'{data_path}/sub_train_data.mat')['sub_train_data']
    # labels_finetue = sio.loadmat(f'{data_path}/sub_train_label.mat')['sub_train_label'][0]
    
    # data_test = sio.loadmat(f'{data_path}/sub_test_data.mat')['sub_data']
    # labels_test = sio.loadmat(f'{data_path}/sub_test_label.mat')['sub_label'][0]
    
    # train_data = data_finetue
    # train_label = labels_finetue - 1
    # test_data = data_test
    # test_label = labels_test - 1
    
    data = sio.loadmat(f'{data_path}/sub_data.mat')['sub_data']
    labels = sio.loadmat(f'{data_path}/sub_label.mat')['sub_label'][0]

    # æ‰“ä¹±ç´¢å¼•
    random_index = np.array(range(len(data)))
    np.random.shuffle(random_index)

    # åº”ç”¨æ‰“ä¹±
    data = data[random_index]
    labels = labels[random_index]

    # åˆ’åˆ†æ•°æ®é›†
    train_len = int(len(data) * train_ratio)

    train_data = data[:train_len]
    test_data = data[train_len:]
    train_label = labels[:train_len] - 1  # æ ‡ç­¾ä»0å¼€å§‹
    test_label = labels[train_len:] - 1

    return train_data, test_data, train_label, test_label


def create_dataloaders(args):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨

    Args:
        args: å‚æ•°å¯¹è±¡

    Returns:
        train_loader, test_loader
    """
    # åŠ è½½æ•°æ®
    train_data, test_data, train_label, test_label = load_and_split_data(
        args.data_path, args.train_ratio
    )

    # åˆ›å»ºæ•°æ®å¢å¼º
    data_transforms = get_data_transforms(
        args.augmentation_prob,
        args.freq_keep_ratio
    )

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = GaitDataset(
        data_array=train_data,
        data_label=train_label,
        data_transform=data_transforms,
        views=2
    )

    test_dataset = GaitDataset(
        data_array=test_data,
        data_label=test_label,
        data_transform=data_transforms,
        views=2
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False
    )

    return train_loader, test_loader


# ================================ æ¨¡å‹å®šä¹‰æ¨¡å— ================================

class SimCLREncoder(nn.Module):
    """SimCLRé£æ ¼çš„ç¼–ç å™¨"""

    def __init__(self, base_model, out_dim=132, proj_out_dim=128, contrastive_dim=256, dropout=0.5):
        """
        Args:
            base_model: åŸºç¡€æ¨¡å‹
            out_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            proj_out_dim: æŠ•å½±å¤´è¾“å‡ºç»´åº¦
            dropout: dropoutæ¦‚ç‡
        """
        super(SimCLREncoder, self).__init__()
        self.encoder = nn.Sequential(*list(base_model.children())[:-1])
        self.dropout = nn.Dropout(p=dropout)

        # æŠ•å½±å¤´ï¼ˆå¯é€‰ï¼‰
        self.projector = nn.Sequential(
            nn.Linear(out_dim, proj_out_dim),
            nn.ReLU(inplace=True),
            nn.Linear(proj_out_dim, contrastive_dim)
        )

    def forward(self, x):
        h = self.encoder(x)
        h = self.dropout(h)
        h = h.view(h.size(0), -1)
        return h


class ClassificationModel(nn.Module):
    """åˆ†ç±»æ¨¡å‹"""

    def __init__(self, encoder, num_features=132, num_classes=27):
        """
        Args:
            encoder: ç¼–ç å™¨
            num_features: ç‰¹å¾ç»´åº¦
            num_classes: åˆ†ç±»æ•°é‡
        """
        super(ClassificationModel, self).__init__()
        self.encoder = encoder
        
        # å¤æ‚çš„å¤šå±‚åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            # ç¬¬å››å±‚ï¼šè¿›ä¸€æ­¥é™ç»´
            nn.Linear(num_features, num_features // 2),
            nn.BatchNorm1d(num_features // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.2),
            
            # è¾“å‡ºå±‚
            nn.Linear(num_features // 2, num_classes)
        )

    def forward(self, x):
        h = self.encoder(x)
        return self.classifier(h)


def get_model(model_type, args, num_classes=27, device='cpu'):
    """
    æ ¹æ®ç±»å‹è·å–æ¨¡å‹

    Args:
        model_type: æ¨¡å‹ç±»å‹
        num_classes: åˆ†ç±»æ•°é‡
        device: è®¾å¤‡

    Returns:
        model: æ¨¡å‹å®ä¾‹
    """
    model_dict = {
        'DNN': DNN,
        'GSDNN': GSDNN_new,
        'ResNet': ResNet18
    }

    if model_type not in model_dict:
        raise ValueError(f"Unsupported model type: {model_type}")

    base_model = model_dict[model_type]()
    encoder = SimCLREncoder(base_model, args.out_dim, args.proj_out_dim, args.contrastive_dim, args.dropout)
    model = ClassificationModel(encoder, num_features=args.out_dim, num_classes=num_classes)

    return model.to(device)


def init_weights(m):
    """Xavieråˆå§‹åŒ–"""
    if isinstance(m, (nn.Conv2d, nn.Linear)):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            m.bias.data.fill_(0.01)


def init_weights_normal(m):
    """æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–"""
    if isinstance(m, (nn.Conv2d, nn.Linear)):
        nn.init.normal_(m.weight, mean=0.0, std=0.01)
        if m.bias is not None:
            m.bias.data.fill_(0.01)


# ================================ è®­ç»ƒå’Œè¯„ä¼°æ¨¡å— ================================

def train_one_epoch(model, dataloader, optimizer, criterion, device, args):
    """
    è®­ç»ƒä¸€ä¸ªepoch

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡

    Returns:
        avg_loss: å¹³å‡æŸå¤±
    """
    model.train()
    total_loss = 0.0
    num_batches = 0

    for x, label in dataloader:
        optimizer.zero_grad()

        # æ•°æ®é¢„å¤„ç†
        x = torch.squeeze(x, dim=1)
        x = x.to(device=device, dtype=torch.float32)
        label = label.to(device=device)

        # å‰å‘ä¼ æ’­
        y_pred = model(x)
        loss = criterion(y_pred, label)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1
        if args.mode == 'debug':
            break

    return total_loss / num_batches


def evaluate(model, dataloader, device):
    """
    è¯„ä¼°æ¨¡å‹

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡

    Returns:
        dict: åŒ…å«å„é¡¹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    model.eval()
    y_preds = []
    y_labels = []
    y_probs = []  # å­˜å‚¨æ¦‚ç‡å€¼

    with torch.no_grad():
        for x, y in dataloader:
            x = torch.squeeze(x, dim=1)
            x = x.to(device=device, dtype=torch.float32)

            # è·å–æ¨¡å‹è¾“å‡ºï¼ˆlogitsï¼‰
            logits = model(x)
            # è·å–é¢„æµ‹ç±»åˆ«
            y_pred = torch.argmax(logits, dim=1).cpu().numpy()
            # è·å–æ¦‚ç‡å€¼ï¼ˆä½¿ç”¨softmaxï¼‰
            y_prob = torch.softmax(logits, dim=1).cpu().numpy()
            y_label = y.cpu().numpy()

            y_preds.append(y_pred)
            y_labels.append(y_label)
            y_probs.append(y_prob)

    y_preds = np.concatenate(y_preds, axis=0)
    y_labels = np.concatenate(y_labels, axis=0)
    y_probs = np.concatenate(y_probs, axis=0)

    # è®¡ç®—æŒ‡æ ‡
    acc = np.mean(np.equal(y_preds, y_labels))
    precision = precision_score(y_labels, y_preds, average='macro', zero_division=0)
    recall = recall_score(y_labels, y_preds, average='macro', zero_division=0)
    f1 = f1_score(y_labels, y_preds, average='macro', zero_division=0)
    
    # æ­£ç¡®è®¡ç®—AUCï¼ˆå¤šåˆ†ç±»æƒ…å†µï¼‰
    try:
        auc_score = roc_auc_score(y_labels, y_probs, multi_class='ovr', average='macro')
    except ValueError:
        # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè®¾ç½®AUCä¸º0
        auc_score = 0.0

    return {
        'predictions': y_preds,
        'labels': y_labels,
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc_score
    }


# ================================ å¯è§†åŒ–æ¨¡å— ================================

def plot_training_curves(losses, train_accs, test_accs, save_path=None):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿

    Args:
        losses: æŸå¤±åˆ—è¡¨
        train_accs: è®­ç»ƒå‡†ç¡®ç‡åˆ—è¡¨
        test_accs: æµ‹è¯•å‡†ç¡®ç‡åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    plt.figure(figsize=(15, 5))

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.subplot(1, 3, 1)
    plt.plot(losses, label='Training Loss')
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 3, 2)
    plt.plot(train_accs, label='Training Accuracy')
    plt.plot(test_accs, label='Testing Accuracy')
    plt.title('Training and Testing Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path)
        print(f"Training curves saved to {save_path}")
    else:
        plt.show()


def plot_confusion_matrix(y_labels, y_preds, save_path=None):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ

    Args:
        y_labels: çœŸå®æ ‡ç­¾
        y_preds: é¢„æµ‹æ ‡ç­¾
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    conf_matrix = confusion_matrix(y_labels, y_preds)

    plt.figure(figsize=(10, 8))
    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()

    num_classes = conf_matrix.shape[0]
    tick_marks = np.arange(num_classes)
    plt.xticks(tick_marks, tick_marks)
    plt.yticks(tick_marks, tick_marks)

    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')

    thresh = conf_matrix.max() / 2.0
    for i, j in np.ndindex(conf_matrix.shape):
        plt.text(j, i, format(conf_matrix[i, j], 'd'),
                horizontalalignment="center",
                color="white" if conf_matrix[i, j] > thresh else "black")

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path)
        print(f"Confusion matrix saved to {save_path}")
    else:
        plt.show()


def log_metrics_to_tensorboard(writer, metrics, epoch, phase='train'):
    """
    å°†æŒ‡æ ‡è®°å½•åˆ°TensorBoard

    Args:
        writer: TensorBoard writer
        metrics: æŒ‡æ ‡å­—å…¸
        epoch: å½“å‰epoch
        phase: é˜¶æ®µï¼ˆtrain/testï¼‰
    """
    prefix = f'{phase}/'

    writer.add_scalar(f'{prefix}Loss', metrics.get('loss', 0), epoch)
    writer.add_scalar(f'{prefix}Accuracy', metrics['accuracy'], epoch)
    writer.add_scalar(f'{prefix}Precision', metrics['precision'], epoch)
    writer.add_scalar(f'{prefix}Recall', metrics['recall'], epoch)
    writer.add_scalar(f'{prefix}F1', metrics['f1'], epoch)
    writer.add_scalar(f'{prefix}AUC', metrics['auc'], epoch)


# ================================ ä¸»è®­ç»ƒæµç¨‹ ================================

def train(args):
    """
    ä¸»è®­ç»ƒå‡½æ•°

    Args:
        args: å‚æ•°å¯¹è±¡
    """
    # è®¾ç½®device
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    Path(args.log_dir).mkdir(parents=True, exist_ok=True)
    Path(args.save_dir).mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("Creating dataloaders...")
    train_loader, test_loader = create_dataloaders(args)
    print(f"Train batches: {len(train_loader)}, Total train data:{len(train_loader)*args.batch_size} \
          Test batches: {len(test_loader)}, Total test data:{len(test_loader)*args.batch_size}")

    # åˆ›å»ºæ¨¡å‹
    print(f"Creating model: {args.model_type}")
    model = get_model(args.model_type, args, args.num_classes, device)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if args.pretrained_model and os.path.exists(args.pretrained_model):
        print(f"Loading pretrained model from {args.pretrained_model}")
        state_dict = torch.load(args.pretrained_model, weights_only=True)
        model.encoder.load_state_dict(state_dict, strict=False)

    # å†»ç»“ç¼–ç å™¨å‚æ•°
    if args.freeze_encoder:
        print("Freezing encoder parameters")
        for param in model.encoder.parameters():
            param.requires_grad = False

    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.learning_rate
    )
    criterion = nn.CrossEntropyLoss()

    # åˆ›å»ºTensorBoard writer
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join(args.log_dir, f"run_{timestamp}")
    writer = SummaryWriter(log_dir)
    print(f"TensorBoard logs will be saved to {log_dir}")

    # è®­ç»ƒå†å²è®°å½•
    history = {
        'losses': [],
        'train_accs': [],
        'train_pres': [],
        'train_recs': [],
        'train_f1s': [],
        'train_aucs': [],
        'test_accs': [],
        'test_pres': [],
        'test_recs': [],
        'test_f1s': [],
        'test_aucs': []
    }

    best_test_acc = 0.0

    # è®­ç»ƒå¾ªç¯
    print(f"\nStarting training for {args.num_epochs} epochs...")
    for epoch in range(args.num_epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss = train_one_epoch(
            model, train_loader, optimizer, criterion, device, args
        )
        history['losses'].append(train_loss)

        # è¯„ä¼°
        train_metrics = evaluate(model, train_loader, device)
        test_metrics = evaluate(model, test_loader, device)

        # è®°å½•å†å²
        history['train_accs'].append(train_metrics['accuracy'])
        history['train_pres'].append(train_metrics['precision'])
        history['train_recs'].append(train_metrics['recall'])
        history['train_f1s'].append(train_metrics['f1'])
        history['train_aucs'].append(train_metrics['auc'])

        history['test_accs'].append(test_metrics['accuracy'])
        history['test_pres'].append(test_metrics['precision'])
        history['test_recs'].append(test_metrics['recall'])
        history['test_f1s'].append(test_metrics['f1'])
        history['test_aucs'].append(test_metrics['auc'])

        # è®°å½•åˆ°TensorBoard
        log_metrics_to_tensorboard(
            writer,
            {**train_metrics, 'loss': train_loss},
            epoch,
            'train'
        )
        log_metrics_to_tensorboard(writer, test_metrics, epoch, 'test')

        # æ‰“å°è¿›åº¦
        print(f'Epoch [{epoch+1}/{args.num_epochs}], '
              f'Loss: {train_loss:.5f}, '
              f'Train Acc: {train_metrics["accuracy"]:.5f}, '
              f'Test Acc: {test_metrics["accuracy"]:.5f}')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_metrics['accuracy'] > best_test_acc:
            best_test_acc = test_metrics['accuracy']
            best_model_path = os.path.join(
                args.save_dir,
                f'best_model_{args.model_type}.pth'
            )
            torch.save(model.state_dict(), best_model_path)
            print(f'New best model saved with accuracy: {best_test_acc:.5f}')

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(
        args.save_dir,
        f'final_model_{args.model_type}.pth'
    )
    torch.save(model.state_dict(), final_model_path)
    print(f'Final model saved to {final_model_path}')

    writer.close()

    # è®¡ç®—æœ€å10ä¸ªepochçš„å¹³å‡æŒ‡æ ‡
    compute_final_metrics(history)

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    # plot_training_curves(
    #     history['losses'],
    #     history['train_accs'],
    #     history['test_accs'],
    #     save_path=os.path.join(args.save_dir, 'training_curves.png')
    # )

    # # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    # plot_confusion_matrix(
    #     test_metrics['labels'],
    #     test_metrics['predictions'],
    #     save_path=os.path.join(args.save_dir, 'confusion_matrix.png')
    # )

    return history, model


def compute_final_metrics(history):
    """
    è®¡ç®—å¹¶æ‰“å°æœ€åNä¸ªepochçš„å¹³å‡æŒ‡æ ‡

    Args:
        history: è®­ç»ƒå†å²å­—å…¸
    """
    last_n = 20
    num_epochs = len(history['test_accs'])

    if num_epochs < last_n:
        print(f"\nWarning: Only {num_epochs} epochs trained, showing all epochs")
        last_n = num_epochs

    print(f"\n{'='*60}")
    print(f"Average metrics of last {last_n} epochs:")
    print(f"{'='*60}")
    print(f"Accuracy:  {np.mean(history['test_accs'][-last_n:]):.5f}")
    print(f"Precision: {np.mean(history['test_pres'][-last_n:]):.5f}")
    print(f"Recall:    {np.mean(history['test_recs'][-last_n:]):.5f}")
    print(f"F1:        {np.mean(history['test_f1s'][-last_n:]):.5f}")
    print(f"AUC:       {np.mean(history['test_aucs'][-last_n:]):.5f}")
    print(f"{'='*60}\n")


# ================================ å‚æ•°è§£æ ================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ­¥æ€è¯†åˆ«åˆ†ç±»è®­ç»ƒ')
    parser.add_argument('--exp_name', type=str, default='Gait_finetune_training')
    parser.add_argument('--mode', type=str, default='debug', help='normal, debug')
    # æ•°æ®ç›¸å…³
    parser.add_argument('--data_path', type=str, default='./datasets/data_10000/', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--train_ratio', type=float, default=0.7, help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')

    # è®­ç»ƒç›¸å…³
    parser.add_argument('--num_epochs', type=int, default=20, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_classes', type=int, default=27, help='åˆ†ç±»æ•°é‡')

    # æ¨¡å‹ç›¸å…³
    parser.add_argument('--model_type', type=str, default='GSDNN',
                       choices=['DNN', 'GSDNN', 'GSDNN2', 'GSDNN_new', 'MSDNN', 'ResNet101'],
                       help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--pretrained_model', type=str,
                       default='./save_models/Gait_self_supervised_training/best_model.pth',
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„') # './save_model/best_modelGSDNNk3_27class_aug123.pth'
    parser.add_argument('--freeze_encoder', action='store_true', help='æ˜¯å¦å†»ç»“ç¼–ç å™¨å‚æ•°')

    ## parameters for projection head
    ### GSDNN [132 128 256]
    ### ResNet18 [64 128 256]
    parser.add_argument('--out_dim', type=int, default=132, help='ç¼–ç å™¨è¾“å‡ºç»´åº¦')
    parser.add_argument('--proj_out_dim', type=int, default=128, help='æŠ•å½±å¤´ä¸­é—´å±‚ç»´åº¦')
    parser.add_argument('--contrastive_dim', type=int, default=256, help='è¿›è¡Œå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾ç©ºé—´ç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.5, help='Dropoutæ¦‚ç‡')
    
    # æ•°æ®å¢å¼ºç›¸å…³
    parser.add_argument('--augmentation_prob', type=float, default=0.5, help='æ•°æ®å¢å¼ºæ¦‚ç‡')
    parser.add_argument('--freq_keep_ratio', type=float, default=0.6, help='é¢‘ç‡æˆåˆ†ä¿ç•™æ¯”ä¾‹')

    # è®¾å¤‡å’Œè·¯å¾„
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡ï¼ˆcuda/cpuï¼‰')
    parser.add_argument('--log_dir', type=str, default='./runs', help='TensorBoardæ—¥å¿—ç›®å½•')
    parser.add_argument('--save_dir', type=str, default='./save_models',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')

    args = parser.parse_args()
    
    # æ‰“å°æ‰€æœ‰é…ç½®ä¿¡æ¯
    print("="*60)
    print("ğŸ“‹ æ­¥æ€è¯†åˆ«è®­ç»ƒé…ç½®ä¿¡æ¯")
    print("="*60)
    
    # æŒ‰ç±»åˆ«åˆ†ç»„æ‰“å°ï¼Œè®©è¾“å‡ºæ›´æ¸…æ™°
    # åŸºç¡€é…ç½®
    print("\n[åŸºç¡€é…ç½®]")
    print(f"  å®éªŒåç§° (exp_name): {args.exp_name}")
    print(f"  è¿è¡Œæ¨¡å¼ (mode): {args.mode}")
    
    # æ•°æ®ç›¸å…³
    print("\n[æ•°æ®ç›¸å…³]")
    print(f"  æ•°æ®è·¯å¾„ (data_path): {args.data_path}")
    print(f"  è®­ç»ƒé›†æ¯”ä¾‹ (train_ratio): {args.train_ratio}")
    print(f"  æ‰¹æ¬¡å¤§å° (batch_size): {args.batch_size}")
    
    # è®­ç»ƒç›¸å…³
    print("\n[è®­ç»ƒç›¸å…³]")
    print(f"  è®­ç»ƒè½®æ•° (num_epochs): {args.num_epochs}")
    print(f"  å­¦ä¹ ç‡ (learning_rate): {args.learning_rate}")
    print(f"  åˆ†ç±»æ•°é‡ (num_classes): {args.num_classes}")
    
    # æ¨¡å‹ç›¸å…³
    print("\n[æ¨¡å‹ç›¸å…³]")
    print(f"  æ¨¡å‹ç±»å‹ (model_type): {args.model_type}")
    print(f"  é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (pretrained_model): {args.pretrained_model}")
    print(f"  å†»ç»“ç¼–ç å™¨ (freeze_encoder): {args.freeze_encoder}")
    print(f"  ç¼–ç å™¨è¾“å‡ºç»´åº¦ (out_dim): {args.out_dim}")
    print(f"  æŠ•å½±å¤´ä¸­é—´å±‚ç»´åº¦ (proj_out_dim): {args.proj_out_dim}")
    print(f"  å¯¹æ¯”å­¦ä¹ ç‰¹å¾ç»´åº¦ (contrastive_dim): {args.contrastive_dim}")
    print(f"  Dropoutæ¦‚ç‡ (dropout): {args.dropout}")
    
    # æ•°æ®å¢å¼ºç›¸å…³
    print("\n[æ•°æ®å¢å¼ºç›¸å…³]")
    print(f"  æ•°æ®å¢å¼ºæ¦‚ç‡ (augmentation_prob): {args.augmentation_prob}")
    print(f"  é¢‘ç‡æˆåˆ†ä¿ç•™æ¯”ä¾‹ (freq_keep_ratio): {args.freq_keep_ratio}")
    
    # è®¾å¤‡å’Œè·¯å¾„
    print("\n[è®¾å¤‡å’Œè·¯å¾„]")
    print(f"  è®¾å¤‡ (device): {args.device}")
    print(f"  TensorBoardæ—¥å¿—ç›®å½• (log_dir): {args.log_dir}")
    print(f"  æ¨¡å‹ä¿å­˜ç›®å½• (save_dir): {args.save_dir}")
    
    print("\n" + "="*60)
    
    return args



# ================================ ä¸»å…¥å£ ================================

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()

    # å¼€å§‹è®­ç»ƒ
    history, model = train(args)

    print("\nTraining completed!")
    print(f"Best test accuracy: {max(history['test_accs']):.5f}")


if __name__ == '__main__':
    main()
