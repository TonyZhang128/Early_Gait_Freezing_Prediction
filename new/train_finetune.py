"""
æ­¥æ€è¯†åˆ«åˆ†ç±»è®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ
åŒ…å«TensorBoardç›‘æ§ã€å‚æ•°è§£æã€æ¨¡å—åŒ–è®¾è®¡
"""

import argparse
import os
from pathlib import Path
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import f1_score, precision_score,recall_score, auc, roc_auc_score
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
import scipy.io as sio



from datasets.Gait_dataset_0202 import GaitDataModule
# å¯¼å…¥æ¨¡å‹
from models.DNN import DNN
from models.GSDNN_new import GSDNN_new
from models.resnet import ResNet18
from models.Conformer import Conformer
from utils.analysis import set_seed
from utils.calculate import FLOPs_calculat
from utils.plt_curves import plot_confusion_matrix

# # ================================ æ•°æ®å¢å¼ºæ¨¡å— ================================

# def reverse_time_series(data):
#     """æ—¶é—´åºåˆ—åè½¬"""
#     return -data


# def random_channel_shuffle(data):
#     """éšæœºé€šé“æ‰“ä¹±"""
#     assert data.dim() == 3, "Input data must be 3D with channels as the second dimension"
#     num_channels = data.size(1)
#     shuffled_indices = torch.randperm(num_channels)
#     return data[:, shuffled_indices, :]


# def random_frequency_dropout(img, keep_ratio=0.6):
#     """éšæœºé¢‘ç‡æˆåˆ†ä¸¢å¼ƒ"""
#     fft_img = torch.fft.fftn(img, dim=2)
#     magnitude = torch.abs(fft_img)
#     num_freqs = magnitude.shape[2]
#     keep_indices = np.random.choice(num_freqs, int(num_freqs * keep_ratio), replace=False)
#     mask = torch.zeros_like(magnitude, dtype=torch.bool)
#     mask[:, :, keep_indices] = 1
#     fft_img = fft_img * mask
#     img = torch.fft.ifftn(fft_img, dim=2)
#     return torch.real(img)


# def get_data_transforms(augmentation_prob=0.5, freq_keep_ratio=0.6):
#     """æ„å»ºæ•°æ®å¢å¼ºå˜æ¢ç»„åˆ"""
#     return transforms.Compose([
#         transforms.RandomApply([
#             transforms.Lambda(lambda x: random_frequency_dropout(x, freq_keep_ratio)),
#             transforms.Lambda(reverse_time_series),
#         ], p=augmentation_prob)
#     ])


# # ================================ æ•°æ®é›†ç±» ================================

# class GaitDataset(Dataset):
#     """æ­¥æ€æ•°æ®é›†ç±»"""

#     def __init__(self, data_array, data_label, data_transform=None, views=2):
#         """
#         Args:
#             data_array: æ•°æ®æ•°ç»„
#             data_label: æ ‡ç­¾æ•°ç»„
#             data_transform: æ•°æ®å˜æ¢
#             views: è§†è§’æ•°é‡
#         """
#         self.transform = data_transform
#         self.data_array = data_array
#         self.data_label = data_label
#         self.views = views

#     def __len__(self):
#         return len(self.data_array)

#     def __getitem__(self, idx):
#         img = self.data_array[idx]
#         if self.transform:
#             img = self.transform(torch.tensor(np.expand_dims(img, axis=0)))
#         return img, self.data_label[idx]


# # ================================ æ•°æ®åŠ è½½æ¨¡å— ================================

# def load_and_split_data(data_path, train_ratio=0.8, random_seed=42):
#     """
#     åŠ è½½å¹¶åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

#     Args:
#         data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«åç¼€ï¼‰
#         train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
#         random_seed: éšæœºç§å­

#     Returns:
#         train_data, test_data, train_label, test_label
#     """
#     np.random.seed(random_seed)

#     # åŠ è½½æ•°æ®
#     # data_finetue = sio.loadmat(f'{data_path}/sub_train_data.mat')['sub_train_data']
#     # labels_finetue = sio.loadmat(f'{data_path}/sub_train_label.mat')['sub_train_label'][0]
    
#     # data_test = sio.loadmat(f'{data_path}/sub_test_data.mat')['sub_data']
#     # labels_test = sio.loadmat(f'{data_path}/sub_test_label.mat')['sub_label'][0]
    
#     # train_data = data_finetue
#     # train_label = labels_finetue - 1
#     # test_data = data_test
#     # test_label = labels_test - 1
    
#     data = sio.loadmat(f'{data_path}/sub_data.mat')['sub_data']
#     labels = sio.loadmat(f'{data_path}/sub_label.mat')['sub_label'][0]

#     # æ‰“ä¹±ç´¢å¼•
#     random_index = np.array(range(len(data)))
#     np.random.shuffle(random_index)

#     # åº”ç”¨æ‰“ä¹±
#     data = data[random_index]
#     labels = labels[random_index]

#     # åˆ’åˆ†æ•°æ®é›†
#     train_len = int(len(data) * train_ratio)

#     train_data = data[:train_len]
#     test_data = data[train_len:]
#     train_label = labels[:train_len] - 1  # æ ‡ç­¾ä»0å¼€å§‹
#     test_label = labels[train_len:] - 1

#     return train_data, test_data, train_label, test_label


# def create_dataloaders(args):
#     """
#     åˆ›å»ºæ•°æ®åŠ è½½å™¨

#     Args:
#         args: å‚æ•°å¯¹è±¡

#     Returns:
#         train_loader, test_loader
#     """
#     # åŠ è½½æ•°æ®
#     train_data, test_data, train_label, test_label = load_and_split_data(
#         args.data_path, args.train_ratio
#     )

#     # åˆ›å»ºæ•°æ®å¢å¼º
#     data_transforms = get_data_transforms(
#         args.augmentation_prob,
#         args.freq_keep_ratio
#     )

#     # åˆ›å»ºæ•°æ®é›†
#     train_dataset = GaitDataset(
#         data_array=train_data,
#         data_label=train_label,
#         data_transform=data_transforms,
#         views=2
#     )

#     test_dataset = GaitDataset(
#         data_array=test_data,
#         data_label=test_label,
#         data_transform=data_transforms,
#         views=2
#     )

#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=args.batch_size,
#         shuffle=True,
#         num_workers=args.num_workers
#     )

#     test_loader = DataLoader(
#         test_dataset,
#         batch_size=args.batch_size,
#         shuffle=False,
#         num_workers=args.num_workers
#     )

#     return train_loader, test_loader

# ================================ æ¨¡å‹å®šä¹‰æ¨¡å— ================================

class SimCLREncoder(nn.Module):
    """SimCLRé£æ ¼çš„ç¼–ç å™¨"""

    def __init__(self, base_model, out_dim=132, proj_out_dim=128, contrastive_dim=256, dropout=0.5):
        """
        Args:
            base_model: åŸºç¡€æ¨¡å‹
            out_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            proj_out_dim: æŠ•å½±å¤´è¾“å‡ºç»´åº¦
            dropout: dropoutæ¦‚ç‡
        """
        super(SimCLREncoder, self).__init__()
        self.encoder = nn.Sequential(*list(base_model.children())[:-1])
        self.dropout = nn.Dropout(p=dropout)

        # æŠ•å½±å¤´ï¼ˆå¯é€‰ï¼‰
        self.projector = nn.Sequential(
            nn.Linear(out_dim, proj_out_dim),
            nn.ReLU(inplace=True),
            nn.Linear(proj_out_dim, contrastive_dim)
        )

    def forward(self, x):
        h = self.encoder(x)
        h = self.dropout(h)
        h = torch.flatten(h, start_dim=1)
        return h


class ClassificationModel(nn.Module):
    """åˆ†ç±»æ¨¡å‹"""

    def __init__(self, encoder, num_features=132, num_classes=27):
        """
        Args:
            encoder: ç¼–ç å™¨
            num_features: ç‰¹å¾ç»´åº¦
            num_classes: åˆ†ç±»æ•°é‡
        """
        super(ClassificationModel, self).__init__()
        self.encoder = encoder
        
        # å¤æ‚çš„å¤šå±‚åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            # ç¬¬å››å±‚ï¼šè¿›ä¸€æ­¥é™ç»´
            nn.Linear(num_features, num_features // 2),
            nn.BatchNorm1d(num_features // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.2),
            
            # è¾“å‡ºå±‚
            nn.Linear(num_features // 2, num_classes)
        )

    def forward(self, x):
        h = self.encoder(x)
        return self.classifier(h)


def get_model(model_type, args, num_classes=27, device='cpu'):
    """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    if model_type == 'GSDNN':
        base_model = GSDNN_new(args.num_classes, args.block_n, args.init_channels, 
                            args.growth_rate, args.base_channels, args.stride, args.dropout_GSDNN)
    elif model_type == 'ResNet':
        base_model = ResNet18()
    elif model_type == 'Conformer':
        base_model = Conformer(emb_size=40, depth=6, n_classes=4)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

    encoder = SimCLREncoder(base_model, args.out_dim, args.proj_out_dim, args.contrastive_dim, args.dropout)
    model = ClassificationModel(encoder, num_features=args.out_dim, num_classes=num_classes)

    return model.to(device)



# ================================ è®­ç»ƒå’Œè¯„ä¼°æ¨¡å— ================================

def train_one_epoch(model, dataloader, optimizer, criterion, device, args):
    """
    è®­ç»ƒä¸€ä¸ªepoch

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡

    Returns:
        avg_loss: å¹³å‡æŸå¤±
    """
    model.train()
    total_loss = 0.0
    num_batches = 0

    for x, label in dataloader:
        optimizer.zero_grad()

        # æ•°æ®é¢„å¤„ç†
        if args.model_type == 'GSDNN':
            x = torch.squeeze(x, dim=1)
        x = x.to(device=device, dtype=torch.float32)
        label = label.to(device=device)

        # å‰å‘ä¼ æ’­
        y_pred = model(x)
        loss = criterion(y_pred, label)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1
        if args.mode == 'debug':
            break

    return total_loss / num_batches


def evaluate(model, dataloader, device):
    """
    è¯„ä¼°æ¨¡å‹

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡

    Returns:
        dict: åŒ…å«å„é¡¹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    model.eval()
    y_preds = []
    y_labels = []
    y_probs = []  # å­˜å‚¨æ¦‚ç‡å€¼

    with torch.no_grad():
        for x, y in dataloader:
            if x.ndim != 4:
                x = torch.squeeze(x, dim=1)
            x = x.to(device=device, dtype=torch.float32)

            # è·å–æ¨¡å‹è¾“å‡ºï¼ˆlogitsï¼‰
            logits = model(x)
            # è·å–é¢„æµ‹ç±»åˆ«
            y_pred = torch.argmax(logits, dim=1).cpu().numpy()
            # è·å–æ¦‚ç‡å€¼ï¼ˆä½¿ç”¨softmaxï¼‰
            y_prob = torch.softmax(logits, dim=1).cpu().numpy()
            y_label = y.cpu().numpy()

            y_preds.append(y_pred)
            y_labels.append(y_label)
            y_probs.append(y_prob)

    y_preds = np.concatenate(y_preds, axis=0)
    y_labels = np.concatenate(y_labels, axis=0)
    y_probs = np.concatenate(y_probs, axis=0)

    # è®¡ç®—æŒ‡æ ‡
    acc = np.mean(np.equal(y_preds, y_labels))
    precision = precision_score(y_labels, y_preds, average='macro', zero_division=0)
    recall = recall_score(y_labels, y_preds, average='macro', zero_division=0)
    f1 = f1_score(y_labels, y_preds, average='macro', zero_division=0)
    
    # æ­£ç¡®è®¡ç®—AUCï¼ˆå¤šåˆ†ç±»æƒ…å†µï¼‰
    try:
        auc_score = roc_auc_score(y_labels, y_probs, multi_class='ovr', average='macro')
    except ValueError:
        # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè®¾ç½®AUCä¸º0
        auc_score = 0.0

    return {
        'predictions': y_preds,
        'labels': y_labels,
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc_score
    }




def log_metrics_to_tensorboard(writer, metrics, epoch, phase='train'):
    """
    å°†æŒ‡æ ‡è®°å½•åˆ°TensorBoard

    Args:
        writer: TensorBoard writer
        metrics: æŒ‡æ ‡å­—å…¸
        epoch: å½“å‰epoch
        phase: é˜¶æ®µï¼ˆtrain/testï¼‰
    """
    prefix = f'{phase}/'

    # ä»…è®­ç»ƒé›†è®°å½•Loss
    if phase == 'train' and 'loss' in metrics:
        writer.add_scalar(f'{prefix}Loss', metrics['loss'], epoch)
        
    writer.add_scalar(f'{prefix}Loss', metrics.get('loss', 0), epoch)
    writer.add_scalar(f'{prefix}Accuracy', metrics['accuracy'], epoch)
    writer.add_scalar(f'{prefix}Precision', metrics['precision'], epoch)
    writer.add_scalar(f'{prefix}Recall', metrics['recall'], epoch)
    writer.add_scalar(f'{prefix}F1', metrics['f1'], epoch)
    writer.add_scalar(f'{prefix}AUC', metrics['auc'], epoch)

        
# ================================ ä¸»è®­ç»ƒæµç¨‹ ================================

def train(args):
    """
    ä¸»è®­ç»ƒå‡½æ•°

    Args:
        args: å‚æ•°å¯¹è±¡
    """
    set_seed(args.seed)
    
    # è®¾ç½®device
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("Creating dataloaders...")
    # train_loader, test_loader = create_dataloaders(args)
    data_module = GaitDataModule(args)
    train_loader, test_loader = data_module.setup()
    print(f"Train batches: {len(train_loader)}, Total train data:{len(train_loader.dataset)} \
          Test batches: {len(test_loader)}, Total test data:{len(test_loader.dataset)}")

    # åˆ›å»ºæ¨¡å‹
    print(f"Creating model: {args.model_type}")
    model = get_model(args.model_type, args, args.num_classes, device)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if args.pretrained_model and os.path.exists(args.pretrained_model):
        print(f"Loading pretrained model from {args.pretrained_model}")
        # state_dict = torch.load(args.pretrained_model, weights_only=True)
        if torch.__version__ >= "1.13.0":
            state_dict = torch.load(args.pretrained_model, weights_only=True)
        else:
            # ä½ç‰ˆæœ¬ç§»é™¤ weights_only å‚æ•°
            state_dict = torch.load(args.pretrained_model)
        # é‡å¤§bugï¼Œä¹‹å‰çš„ä»£ç æ²¡æœ‰æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡ï¼
        model.encoder.load_state_dict(state_dict['model_state_dict'], strict=False)

    # å†»ç»“ç¼–ç å™¨å‚æ•°
    if args.freeze_encoder:
        print("Freezing encoder parameters")
        for param in model.encoder.parameters():
            param.requires_grad = False

        # åˆå§‹åŒ–åˆ†ç±»å¤´å‚æ•°
        print("Initializing classifier parameters...")
        for m in model.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
                
    # è®¡ç®—æ¨¡å‹å‚æ•°é‡ä¸FLOPS
    data_shape = [1,1, 18, 101]
    FLOPs_calculat(model, device, data_shape)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.learning_rate
    )
    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, 
    #                                             gamma=args.lr_gamma)

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=args.num_epochs,  # ä½™å¼¦å‘¨æœŸï¼šå­¦ä¹ ç‡ä»åˆå§‹å€¼è¡°å‡åˆ°æœ€å°å€¼çš„è½®æ•°ï¼Œè®¾ä¸ºæ€»è®­ç»ƒè½®æ•°æœ€ä½³
                eta_min=1e-6        # å­¦ä¹ ç‡è¡°å‡çš„æœ€å°å€¼ï¼ˆé¿å…è¡°å‡åˆ°0å¯¼è‡´è®­ç»ƒåœæ»ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
            )

    criterion = nn.CrossEntropyLoss()

    # åˆ›å»ºTensorBoard writer
    # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # log_dir = os.path.join(args.log_dir, args.exp_name + '_' + timestamp)
    # save_dir = os.path.join(args.save_dir, args.exp_name + '_' + timestamp)
    log_dir = args.log_dir
    save_dir = args.save_dir
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)
    
    
    writer = SummaryWriter(log_dir)
    print(f"TensorBoard logs will be saved to {log_dir}")
    print(f'Models will be saved to: {save_dir}')

    # è®­ç»ƒå†å²è®°å½•
    history = {
        'losses': [],
        'train_accs': [],
        'train_pres': [],
        'train_recs': [],
        'train_f1s': [],
        'train_aucs': [],
        'test_accs': [],
        'test_pres': [],
        'test_recs': [],
        'test_f1s': [],
        'test_aucs': []
    }

    best_test_acc = 0.0

    # è®­ç»ƒå¾ªç¯
    print(f"\nStarting training for {args.num_epochs} epochs...")
    for epoch in range(args.num_epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss = train_one_epoch(
            model, train_loader, optimizer, criterion, device, args
        )
        history['losses'].append(train_loss)

        # è¯„ä¼°
        train_metrics = evaluate(model, train_loader, device)
        test_metrics = evaluate(model, test_loader, device)

        # è®°å½•å†å²
        history['train_accs'].append(train_metrics['accuracy'])
        history['train_pres'].append(train_metrics['precision'])
        history['train_recs'].append(train_metrics['recall'])
        history['train_f1s'].append(train_metrics['f1'])
        history['train_aucs'].append(train_metrics['auc'])

        history['test_accs'].append(test_metrics['accuracy'])
        history['test_pres'].append(test_metrics['precision'])
        history['test_recs'].append(test_metrics['recall'])
        history['test_f1s'].append(test_metrics['f1'])
        history['test_aucs'].append(test_metrics['auc'])

        # è®°å½•åˆ°TensorBoard
        log_metrics_to_tensorboard(
            writer,
            {**train_metrics, 'loss': train_loss},
            epoch,
            'train'
        )
        log_metrics_to_tensorboard(writer, test_metrics, epoch, 'test')

        # æ‰“å°è¿›åº¦
        print(f'Epoch [{epoch+1}/{args.num_epochs}], '
              f'Loss: {train_loss:.5f}, '
              f'Train Acc: {train_metrics["accuracy"]:.5f}, '
              f'Test Acc: {test_metrics["accuracy"]:.5f}')

        current_lr = optimizer.param_groups[0]['lr']
        writer.add_scalar('train/LearningRate', current_lr, epoch)
        scheduler.step()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_metrics['accuracy'] > best_test_acc:
            best_test_acc = test_metrics['accuracy']
            best_model_path = os.path.join(
                save_dir,
                f'best_model_{args.model_type}.pth'
            )
            torch.save(model.state_dict(), best_model_path)
            print(f'New best model saved with accuracy: {best_test_acc:.5f}')

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(
        save_dir,
        f'final_acc_model_{args.model_type}.pth'
    )
    torch.save(model.state_dict(), final_model_path)
    print(f'Final model saved to {final_model_path}')

    writer.close()

    # è®¡ç®—æœ€å10ä¸ªepochçš„å¹³å‡æŒ‡æ ‡
    compute_final_metrics(history)
    
    print(f'All results saved to: {save_dir}')
    print(f"View TensorBoard logs with: tensorboard --logdir={log_dir}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    # plot_training_curves(
    #     history['losses'],
    #     history['train_accs'],
    #     history['test_accs'],
    #     save_path=os.path.join(args.save_dir, 'training_curves.png')
    # )

    # # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(
        test_metrics['labels'],
        test_metrics['predictions'],
        save_path=os.path.join(args.save_dir, 'confusion_matrix.png')
    )

    return history, model


def compute_final_metrics(history):
    """
    è®¡ç®—å¹¶æ‰“å°æœ€åNä¸ªepochçš„å¹³å‡æŒ‡æ ‡

    Args:
        history: è®­ç»ƒå†å²å­—å…¸
    """
    last_n = 20
    num_epochs = len(history['test_accs'])

    if num_epochs < last_n:
        print(f"\nWarning: Only {num_epochs} epochs trained, showing all epochs")
        last_n = num_epochs

    print(f"\n{'='*60}")
    print(f"Average metrics of last {last_n} epochs:")
    print(f"{'='*60}")
    print(f"Accuracy:  {np.mean(history['test_accs'][-last_n:]):.5f}")
    print(f"Precision: {np.mean(history['test_pres'][-last_n:]):.5f}")
    print(f"Recall:    {np.mean(history['test_recs'][-last_n:]):.5f}")
    print(f"F1:        {np.mean(history['test_f1s'][-last_n:]):.5f}")
    print(f"AUC:       {np.mean(history['test_aucs'][-last_n:]):.5f}")
    print(f"{'='*60}\n")


# ================================ å‚æ•°è§£æ ================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ­¥æ€è¯†åˆ«åˆ†ç±»è®­ç»ƒ')
    parser.add_argument('--exp_name', type=str, default='Gait_finetune_training')
    parser.add_argument('--mode', type=str, default='debug', help='normal, debug')
    parser.add_argument('--print_params', type=bool, default=True, help='æ‰“å°å‚æ•°')

    # æ•°æ®ç›¸å…³
    parser.add_argument('--data_path', type=str, default='./datasets/data_10000/', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--train_ratio', type=float, default=0.7, help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=0, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')

    # è®­ç»ƒç›¸å…³
    parser.add_argument('--num_epochs', type=int, default=20, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=2e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_classes', type=int, default=27, help='åˆ†ç±»æ•°é‡')
    parser.add_argument('--lr_step_size', type=int, default=10, help='StepLRå­¦ä¹ ç‡è¡°å‡æ­¥é•¿')
    parser.add_argument('--lr_gamma', type=float, default=0.5, help='StepLRå­¦ä¹ ç‡è¡°å‡ç³»æ•°')

    # æ¨¡å‹ç›¸å…³
    parser.add_argument('--model_type', type=str, default='ResNet',
                       choices=['DNN', 'GSDNN', 'GSDNN2', 'GSDNN_new', 'MSDNN', 'ResNet101'],
                       help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--pretrained_model', type=str,
                       default='./save_models/Gait_selfsup_GSDNN_baseline_20260131_192518/best_model.pth',
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„') # './save_model/best_modelGSDNNk3_27class_aug123.pth'
    parser.add_argument('--freeze_encoder', action='store_true', help='æ˜¯å¦å†»ç»“ç¼–ç å™¨å‚æ•°')

    ## parameters for GSDNN
    parser.add_argument('--block_n', type=int, default=8, help='æ¨¡å—å †å æ¬¡æ•°')
    parser.add_argument('--init_channels', type=int, default=18, help='æ•°æ®è¾“å…¥ç»´åº¦')
    parser.add_argument('--growth_rate', type=int, default=12, help='æ¨¡å—æ¯å ä¸€æ¬¡ï¼Œç»´åº¦æå‡å¤šå°‘')
    parser.add_argument('--base_channels', type=int, default=48, help='åˆå§‹ç‰¹å¾ç»´åº¦')
    parser.add_argument('--stride', type=int, default=2, help='å·ç§¯æ­¥é•¿')
    parser.add_argument('--dropout_GSDNN', type=float, default=0.2, help='GSDNNä¸¢å¤±æ¦‚ç‡')
    
    ## parameters for projection head
    ### GSDNN [132 128 256]
    ### ResNet18 [512 1024 128]
    parser.add_argument('--out_dim', type=int, default=512, help='ç¼–ç å™¨è¾“å‡ºç»´åº¦')
    parser.add_argument('--proj_out_dim', type=int, default=1024, help='æŠ•å½±å¤´ä¸­é—´å±‚ç»´åº¦')
    parser.add_argument('--contrastive_dim', type=int, default=128, help='è¿›è¡Œå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾ç©ºé—´ç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropoutæ¦‚ç‡')
    
    # æ•°æ®å¢å¼ºç›¸å…³
    parser.add_argument('--augmentation_prob', type=float, default=0.5, help='æ•°æ®å¢å¼ºæ¦‚ç‡')
    parser.add_argument('--freq_keep_ratio', type=float, default=0.6, help='é¢‘ç‡æˆåˆ†ä¿ç•™æ¯”ä¾‹')

    # è®¾å¤‡å’Œè·¯å¾„
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡ï¼ˆcuda/cpuï¼‰')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--log_dir', type=str, default='./runs', help='TensorBoardæ—¥å¿—ç›®å½•')
    parser.add_argument('--save_dir', type=str, default='./save_models',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•')

    args = parser.parse_args()
    
    # æ‰“å°æ‰€æœ‰é…ç½®ä¿¡æ¯
    if args.print_params:
        print("="*70)
        print("ğŸ“Š æ­¥æ€è¯†åˆ«åˆ†ç±»è®­ç»ƒé…ç½®ä¿¡æ¯")
        print("="*70)
        for key, value in sorted(vars(args).items()):
            print(f"  {key.ljust(30)}: {value}")
        print("="*70)
    
    return args



# ================================ ä¸»å…¥å£ ================================

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()

    # å¼€å§‹è®­ç»ƒ
    history, model = train(args)

    print("\nTraining completed!")
    print(f"Best test accuracy: {max(history['test_accs']):.5f}")


if __name__ == '__main__':
    main()
